{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW4 - HAC\n",
    "* Documents are represented as normalized tf-idf vectors.\n",
    "* Cosine similarity for pair-wise document similarity.\n",
    "* Similarity measure between clusters can be:\n",
    "    * single-link, complete-link, group-average, and centroid similarity.\n",
    "* To speed up your clustering … you MAY … (15 bonus points)\n",
    "    * Use YOUR OWN  to obtain the cluster pair with maximal similarity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import math\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "p = PorterStemmer()\n",
    "stemmed_doc =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')\n",
    "os.chdir('./Data/IRTM/')\n",
    "my_files = glob.glob('*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = '''!()-[]{};:'`\"\\,<>./?@#$%^&*_~'''\n",
    "numbers ='1234567890'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize( doc : str ) -> list:\n",
    "\n",
    "    punctuations = '''!()-[]{};:'`\"\\,<>./?@#$%^&*_~'''\n",
    "    numbers ='1234567890'\n",
    "        \n",
    "    # 處理標點符號與大小寫, 句子沒有斷乾淨\n",
    "    for punc in punctuations:\n",
    "        doc = doc.replace(punc, ' ').replace('\\n','')\n",
    "    for num in numbers:\n",
    "        doc = doc.replace(num,'')\n",
    "        \n",
    "    doc_list = doc.lower().split(' ')\n",
    "        \n",
    "    stemmed_doc = [p.stem(token) for token in doc_list if p.stem(token)!= '\\n' and not p.stem(token).isnumeric() and len(p.stem(token))>1 ]\n",
    "        \n",
    "    return stemmed_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_and_df( my_files : list) -> dict :\n",
    "\n",
    "    tf_dict = {} # {'idx' : {'token':tf score}}\n",
    "    df_dict = {} # {'tok' : df score}\n",
    "\n",
    "    for file in my_files:\n",
    "        idx = file.strip('.txt')\n",
    "        \n",
    "        tf_dict[idx] = {}\n",
    "\n",
    "        with open(file) as f:\n",
    "            lines = f.read()\n",
    "\n",
    "        stemmed_doc = tokenize(lines)\n",
    "\n",
    "        N = len(stemmed_doc)\n",
    "\n",
    "        # create tf_dict\n",
    "        for tok in stemmed_doc:\n",
    "            if tok not in tf_dict[idx]:\n",
    "                tf_dict[idx][tok] = 1\n",
    "            else:\n",
    "                tf_dict[idx][tok] += 1\n",
    "        \n",
    "        # create df\n",
    "        for tok in set(stemmed_doc):\n",
    "            if tok not in df_dict:\n",
    "                df_dict[tok] = 1\n",
    "            else:\n",
    "                df_dict[tok] += 1\n",
    "\n",
    "    return tf_dict , df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_idf_vec( tf_dict : dict , df_dict : dict ) -> dict :\n",
    "\n",
    "    tf_idf_dict = {} # {'idx' : {'token':tf_idf score}} \n",
    "\n",
    "    N = len(tf_dict)\n",
    "\n",
    "    for idx in tf_dict:\n",
    "        tf_idf_dict[idx]={}\n",
    "\n",
    "        # normalization \n",
    "        total = sum(tf_dict[idx].values())\n",
    "        tf_dict[idx] = {key: value / total for key, value in tf_dict[idx].items()}\n",
    "        \n",
    "        for tok in tf_dict[idx]:\n",
    "            tf_idf_dict[idx][tok] = tf_dict[idx][tok] * math.log( N / df_dict[tok] )\n",
    "\n",
    "    return tf_idf_dict\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim( idx_1 : int  ,idx_2 : int , df_dict : dict , tf_idf_dict : dict) -> int:\n",
    "\n",
    "    idx_1 = str(idx_1)\n",
    "    idx_2 = str(idx_2)\n",
    "\n",
    "    bag_of_words = set(list(tf_idf_dict[idx_1].keys())+list(tf_idf_dict[idx_2].keys()))\n",
    "\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for word in bag_of_words:\n",
    "        if word in tf_idf_dict[idx_1]:\n",
    "            x_list.append(tf_idf_dict[idx_1][word])\n",
    "        elif word not in  tf_idf_dict[idx_1]:\n",
    "            x_list.append(0)\n",
    "            \n",
    "        if word in tf_idf_dict[idx_2]:\n",
    "            y_list.append(tf_idf_dict[idx_2][word])\n",
    "        elif word not in  tf_idf_dict[idx_2]:\n",
    "            y_list.append(0)\n",
    "\n",
    "        \n",
    "    \n",
    "    return dot(x_list, y_list)/(norm(x_list)*norm(y_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_dict , df_dict = get_tf_and_df( my_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_dict = create_tf_idf_vec( tf_dict , df_dict )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.203964240589032\n"
     ]
    }
   ],
   "source": [
    "print( cos_sim( 1 , 2, df_dict , tf_idf_dict) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(tf_idf_dict)\n",
    "C = np.array([[cos_sim(idx_1, idx_2 , df_dict , tf_idf_dict) for idx_1 in range(1, N+1)] for idx_2 in range(1, N+1)])\n",
    "I = np.array([1]* N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_cos( N, C, I ):\n",
    "\n",
    "    max_sim = -1\n",
    "    doc_i, doc_m = -1, -1\n",
    "    for i in range(N):\n",
    "        if I[i] != 1:\n",
    "            continue\n",
    "        for m in range(N):\n",
    "            if I[m] == 1 and i != m:\n",
    "                if max_sim < C[i][m]:\n",
    "                    max_sim = C[i][m]\n",
    "                    doc_i, doc_m = i, m\n",
    "    return doc_i, doc_m\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.20396424, 0.30235042, ..., 0.05642569, 0.03213732,\n",
       "        0.05919668],\n",
       "       [0.20396424, 1.        , 0.21476783, ..., 0.04307513, 0.01393332,\n",
       "        0.02685205],\n",
       "       [0.30235042, 0.21476783, 1.        , ..., 0.0605471 , 0.02932991,\n",
       "        0.05163034],\n",
       "       ...,\n",
       "       [0.05642569, 0.04307513, 0.0605471 , ..., 1.        , 0.13302009,\n",
       "        0.03209313],\n",
       "       [0.03213732, 0.01393332, 0.02932991, ..., 0.13302009, 1.        ,\n",
       "        0.02279992],\n",
       "       [0.05919668, 0.02685205, 0.05163034, ..., 0.03209313, 0.02279992,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for k in range(N-1):\n",
    "    A = []\n",
    "    i, m = get_max_cos( N, C, I )\n",
    "    A.append([i ,m])\n",
    "    for j in range(N):\n",
    "        tmp = min(cos_sim(i+1, j+1, df_dict , tf_idf_dict), cos_sim(m+1, j+1, df_dict , tf_idf_dict))\n",
    "        C[i][j] = tmp\n",
    "        C[j][i] = tmp\n",
    "    I[m] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_result(hac_dict, cluster_num):\n",
    "    with open(f\"./{cluster_num}.txt\", \"w\") as f:\n",
    "        for k, v in hac_dict.items():\n",
    "            for doc_id in sorted(v):\n",
    "                f.write(f\"{doc_id+1}\\n\")\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "hac_dict = {str(i) : [i] for i in range(N)}\n",
    "for doc_i, doc_m in A:\n",
    "    new_element = hac_dict[str(doc_m)]\n",
    "    hac_dict.pop(str(doc_m))\n",
    "    hac_dict[str(doc_i)] += new_element\n",
    "    if len(hac_dict) == 20:\n",
    "        write_result(hac_dict, 20)\n",
    "    if len(hac_dict) == 13:\n",
    "        write_result(hac_dict, 13)\n",
    "    if len(hac_dict) == 8:\n",
    "        write_result(hac_dict, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
